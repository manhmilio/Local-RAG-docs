# ğŸ¥ Medical Chatbot - Há»‡ Thá»‘ng Cháº©n ÄoÃ¡n Bá»‡nh AI

Há»‡ thá»‘ng chatbot y táº¿ thÃ´ng minh sá»­ dá»¥ng RAG (Retrieval Augmented Generation) vá»›i Ollama LLM vÃ  ChromaDB vector store.

## ğŸ“‹ Tá»•ng Quan

### CÃ´ng Nghá»‡ Stack

**Backend:**
- FastAPI 0.104+ (Python 3.10+)
- Ollama (mistral:7b hoáº·c llama3.1:8b)
- ChromaDB (Vector Database)
- LangChain Framework
- Sentence-Transformers (Embeddings)
- PyPDF2 (PDF Processing)

**Frontend:**
- React 18+
- Tailwind CSS 3+
- Vite (Build Tool)
- Axios (HTTP Client)
- React Markdown (Markdown Rendering)
- Lucide React (Icons)

### TÃ­nh NÄƒng ChÃ­nh

âœ… Chat interface hiá»‡n Ä‘áº¡i vá»›i streaming responses  
âœ… RAG system vá»›i ChromaDB vector search  
âœ… Upload vÃ  xá»­ lÃ½ tÃ i liá»‡u PDF y táº¿  
âœ… Há»— trá»£ tiáº¿ng Viá»‡t tá»‘t  
âœ… Semantic search vá»›i embeddings multilingual  
âœ… Real-time health check vÃ  monitoring  
âœ… LÆ°u trá»¯ lá»‹ch sá»­ chat  
âœ… Markdown formatting cho responses  

---

## ğŸš€ HÆ°á»›ng Dáº«n CÃ i Äáº·t

### BÆ°á»›c 1: CÃ i Äáº·t Ollama

#### Windows:
1. Download Ollama tá»«: https://ollama.ai/download
2. Cháº¡y installer vÃ  cÃ i Ä‘áº·t
3. Má»Ÿ PowerShell vÃ  pull model:

```powershell
ollama pull mistral:7b
# Hoáº·c
ollama pull llama3.1:8b
```

4. Kiá»ƒm tra Ollama Ä‘ang cháº¡y:
```powershell
ollama list
```

#### Linux/Mac:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull mistral:7b
```

### BÆ°á»›c 2: Setup Backend

1. **Táº¡o Python Virtual Environment:**

```powershell
cd backend
python -m venv venv
.\venv\Scripts\Activate.ps1  # Windows
# source venv/bin/activate    # Linux/Mac
```

2. **CÃ i Ä‘áº·t Dependencies:**

```powershell
pip install --upgrade pip
pip install -r requirements.txt
```

3. **Cáº¥u hÃ¬nh Environment Variables:**

```powershell
# Copy file .env.example
copy ..\.env.example .env

# Chá»‰nh sá»­a .env náº¿u cáº§n (máº·c Ä‘á»‹nh Ä‘Ã£ OK cho local)
```

4. **Khá»Ÿi Ä‘á»™ng Backend Server:**

```powershell
python main.py
```

Server sáº½ cháº¡y táº¡i: http://localhost:8000  
API Docs: http://localhost:8000/docs

### BÆ°á»›c 3: Setup Frontend

1. **CÃ i Ä‘áº·t Node.js Dependencies:**

```powershell
cd ..\frontend
npm install
```

2. **Cáº¥u hÃ¬nh Environment:**

```powershell
copy .env.example .env
```

3. **Khá»Ÿi Ä‘á»™ng Development Server:**

```powershell
npm run dev
```

Frontend sáº½ cháº¡y táº¡i: http://localhost:3000

---

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
Local-RAG/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI application entry point
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ models.py            # Pydantic data models
â”‚   â”œâ”€â”€ vector_store.py      # ChromaDB integration
â”‚   â”œâ”€â”€ llm_service.py       # Ollama LLM service
â”‚   â”œâ”€â”€ pdf_processor.py     # PDF processing module
â”‚   â””â”€â”€ requirements.txt     # Python dependencies
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/      # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatMessage.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInput.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Header.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ TypingIndicator.jsx
â”‚   â”‚   â”‚   â””â”€â”€ UploadModal.jsx
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js       # API service
â”‚   â”‚   â”œâ”€â”€ App.jsx          # Main app component
â”‚   â”‚   â”œâ”€â”€ main.jsx         # Entry point
â”‚   â”‚   â””â”€â”€ index.css        # Global styles
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â””â”€â”€ tailwind.config.js
â”‚
â”œâ”€â”€ data/                    # PDF documents folder
â”œâ”€â”€ chroma_db/              # ChromaDB persistent storage
â””â”€â”€ .env.example            # Environment variables template
```

---

## ğŸ”§ Cáº¥u HÃ¬nh Chi Tiáº¿t

### Backend Configuration (.env)

```env
# Server
HOST=0.0.0.0
PORT=8000
RELOAD=True

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral:7b

# ChromaDB
CHROMA_PERSIST_DIRECTORY=./chroma_db
CHROMA_COLLECTION_NAME=medical_documents

# Embeddings
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2

# LLM Parameters
TEMPERATURE=0.3          # Äá»™ sÃ¡ng táº¡o (0-1)
MAX_TOKENS=2048          # Max tokens response
TOP_P=0.9               # Nucleus sampling

# RAG Settings
TOP_K_RESULTS=3          # Sá»‘ documents retrieve
SIMILARITY_THRESHOLD=0.7 # NgÆ°á»¡ng similarity

# CORS
CORS_ORIGINS=http://localhost:3000,http://localhost:5173
```

### Frontend Configuration (.env)

```env
VITE_API_URL=http://localhost:8000
```

---

## ğŸ“š Sá»­ Dá»¥ng

### 1. Upload TÃ i Liá»‡u PDF

1. Click nÃºt "Upload PDF" trÃªn header
2. Chá»n file PDF y táº¿ (sÃ¡ch, bÃ i bÃ¡o, tÃ i liá»‡u cháº©n Ä‘oÃ¡n...)
3. Há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng:
   - Extract text tá»« PDF
   - Chia thÃ nh chunks
   - Táº¡o embeddings
   - LÆ°u vÃ o ChromaDB

### 2. Chat vá»›i AI

1. GÃµ cÃ¢u há»i vÃ o Ã´ input
2. Há»‡ thá»‘ng sáº½:
   - TÃ¬m kiáº¿m documents liÃªn quan (RAG)
   - Gá»­i context + question Ä‘áº¿n LLM
   - Stream response real-time
3. Xem nguá»“n tham kháº£o Ä‘Æ°á»£c hiá»ƒn thá»‹ cÃ¹ng cÃ¢u tráº£ lá»i

### 3. API Endpoints

#### Health Check
```bash
GET /health
```

#### Chat (Non-streaming)
```bash
POST /chat
Content-Type: application/json

{
  "message": "Triá»‡u chá»©ng cá»§a bá»‡nh cáº£m cÃºm lÃ  gÃ¬?",
  "use_rag": true,
  "conversation_history": []
}
```

#### Chat (Streaming)
```bash
POST /chat/stream
Content-Type: application/json

{
  "message": "Triá»‡u chá»©ng cá»§a bá»‡nh cáº£m cÃºm lÃ  gÃ¬?",
  "use_rag": true
}
```

#### Upload Document
```bash
POST /documents/upload
Content-Type: multipart/form-data

file: <PDF_FILE>
```

#### Get Stats
```bash
GET /documents/stats
```

#### Reindex All PDFs
```bash
POST /documents/reindex
```

---

## ğŸ› ï¸ Troubleshooting

### Backend Issues

**Problem: "Connection to Ollama failed"**
```powershell
# Kiá»ƒm tra Ollama service
ollama list

# Restart Ollama (Windows)
# Táº¯t vÃ  má»Ÿ láº¡i Ollama app

# Test connection
curl http://localhost:11434/api/tags
```

**Problem: "ChromaDB error"**
```powershell
# XÃ³a vÃ  táº¡o láº¡i database
rm -r chroma_db
# Restart backend - sáº½ tá»± táº¡o láº¡i
```

**Problem: "Model not found"**
```powershell
# Pull model
ollama pull mistral:7b
```

**Problem: "Import errors"**
```powershell
# Reinstall dependencies
pip install --force-reinstall -r requirements.txt
```

### Frontend Issues

**Problem: "API connection failed"**
- Kiá»ƒm tra backend Ä‘ang cháº¡y táº¡i port 8000
- Kiá»ƒm tra CORS settings
- Xem console logs

**Problem: "Module not found"**
```powershell
# Clear cache vÃ  reinstall
rm -r node_modules
rm package-lock.json
npm install
```

**Problem: "Build errors"**
```powershell
npm run build
# Check console for specific errors
```

---

## ğŸ¯ Tá»‘i Æ¯u Hiá»‡u Suáº¥t

### 1. LLM Parameters

Chá»‰nh sá»­a trong `.env`:
```env
TEMPERATURE=0.3      # â†“ = chÃ­nh xÃ¡c hÆ¡n, â†‘ = sÃ¡ng táº¡o hÆ¡n
MAX_TOKENS=2048      # TÄƒng cho responses dÃ i hÆ¡n
TOP_P=0.9           # Nucleus sampling
```

### 2. RAG Settings

```env
TOP_K_RESULTS=3              # Sá»‘ documents retrieve
SIMILARITY_THRESHOLD=0.7     # NgÆ°á»¡ng minimum similarity
```

### 3. Chunking Strategy

Trong `pdf_processor.py`:
```python
self.chunk_size = 1000       # KÃ­ch thÆ°á»›c chunk
self.chunk_overlap = 200     # Overlap giá»¯a chunks
```

### 4. Embedding Model

CÃ³ thá»ƒ thay Ä‘á»•i model trong `.env`:
```env
# Faster but less accurate
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Better for Vietnamese
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2

# Best quality (slower)
EMBEDDING_MODEL=paraphrase-multilingual-mpnet-base-v2
```

---

## ğŸ”’ Production Deployment

### 1. Security Checklist

- [ ] Thay Ä‘á»•i CORS origins
- [ ] ThÃªm authentication/authorization
- [ ] Rate limiting
- [ ] Input validation & sanitization
- [ ] HTTPS only
- [ ] Environment variables tá»« secrets manager

### 2. Docker Deployment

#### Backend Dockerfile
```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Frontend Dockerfile
```dockerfile
FROM node:18-alpine AS build

WORKDIR /app
COPY package*.json ./
RUN npm ci

COPY . .
RUN npm run build

FROM nginx:alpine
COPY --from=build /app/dist /usr/share/nginx/html
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

#### Docker Compose
```yaml
version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./chroma_db:/app/chroma_db
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434

  frontend:
    build: ./frontend
    ports:
      - "3000:80"
    depends_on:
      - backend
```

### 3. Monitoring

ThÃªm logging vÃ  metrics:
```python
# backend/main.py
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)
```

---

## ğŸ“Š Performance Benchmarks

### Expected Performance

- **Embedding generation**: ~50ms per chunk
- **Vector search**: ~10-30ms for 1000 documents
- **LLM response (streaming)**: First token ~500ms, subsequent ~50ms/token
- **PDF processing**: ~2-5s per page

### Scaling Recommendations

- **< 1000 documents**: Current setup OK
- **1000-10000 documents**: Consider Postgres with pgvector
- **> 10000 documents**: Use Pinecone/Weaviate/Qdrant
- **High traffic**: Add load balancer, multiple Ollama instances

---

## ğŸ¤ Contributing

### Development Setup

1. Fork repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Make changes
4. Test thoroughly
5. Commit: `git commit -m 'Add amazing feature'`
6. Push: `git push origin feature/amazing-feature`
7. Create Pull Request

### Code Style

- Python: Follow PEP 8
- JavaScript: ESLint config included
- Comments: Tiáº¿ng Viá»‡t cho business logic

---

## ğŸ“ License

MIT License - xem file LICENSE

---

## ğŸ†˜ Support

**Issues**: Create issue trÃªn GitHub  
**Discussions**: GitHub Discussions  
**Email**: support@example.com

---

## ğŸ™ Credits

- **Ollama**: https://ollama.ai
- **LangChain**: https://langchain.com
- **ChromaDB**: https://www.trychroma.com
- **FastAPI**: https://fastapi.tiangolo.com
- **React**: https://react.dev

---

## ğŸ“… Roadmap

- [ ] User authentication & authorization
- [ ] Multi-user support vá»›i isolated vector stores
- [ ] Advanced medical NER (Named Entity Recognition)
- [ ] Integration vá»›i medical databases (ICD-10, SNOMED)
- [ ] Voice input/output
- [ ] Mobile app (React Native)
- [ ] Multilingual support (English, Vietnamese, etc.)
- [ ] A/B testing different LLM models
- [ ] Analytics dashboard
- [ ] Export chat history

---

**Version**: 1.0.0  
**Last Updated**: January 2026  
**Author**: Medical AI Team
