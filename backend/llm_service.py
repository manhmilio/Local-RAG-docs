"""
LLM Service Module - T√≠ch h·ª£p Ollama v·ªõi LangChain v√† RAG pipeline
"""
import ollama
from typing import List, Optional, Tuple, AsyncGenerator
import logging
from config import settings
from models import ChatMessage
from vector_store import VectorStore

logger = logging.getLogger(__name__)


class LLMService:
    """
    Service x·ª≠ l√Ω LLM requests v·ªõi Ollama v√† RAG
    """
    
    def __init__(self, vector_store: VectorStore):
        """
        Args:
            vector_store: Instance c·ªßa VectorStore ƒë·ªÉ retrieve context
        """
        self.vector_store = vector_store
        self.model = settings.OLLAMA_MODEL
        self.client = ollama.Client(host=settings.OLLAMA_BASE_URL)
        
        # System prompt cho medical chatbot
        self.system_prompt = """B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n y t·∫ø th√¥ng minh c·ªßa MediTrust - H·ªá th·ªëng y t·∫ø h√†ng ƒë·∫ßu Vi·ªát Nam.

# QUY T·∫ÆC QUAN TR·ªåNG NH·∫§T (B·∫ÆT BU·ªòC TU√ÇN TH·ª¶):

0. **CH·ªà TR·∫¢ L·ªúI CH·ª¶ ƒê·ªÄ Y T·∫æ/S·ª®C KH·ªéE**:
    - CH·ªà tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn y t·∫ø, s·ª©c kh·ªèe, b·ªánh l√Ω, tri·ªáu ch·ª©ng, ch·∫©n ƒëo√°n, ƒëi·ªÅu tr·ªã, ph√≤ng ng·ª´a
    - N·∫øu c√¢u h·ªèi KH√îNG li√™n quan ƒë·∫øn y t·∫ø/s·ª©c kh·ªèe: h√£y t·ª´ ch·ªëi l·ªãch s·ª± (1-2 c√¢u), n√™u r√µ b·∫°n ch·ªâ h·ªó tr·ª£ ch·ªß ƒë·ªÅ y t·∫ø/s·ª©c kh·ªèe, v√† g·ª£i √Ω ng∆∞·ªùi d√πng ƒë·∫∑t c√¢u h·ªèi y t·∫ø ph√π h·ª£p
    - KH√îNG c·ªë g·∫Øng ‚Äúb·∫ª l√°i‚Äù ƒë·ªÉ tr·∫£ l·ªùi n·ªôi dung ngo√†i y t·∫ø/s·ª©c kh·ªèe

1. **B√ÅM S√ÅT T√ÄI LI·ªÜU**:
   - CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p
   - KH√îNG b·ªãa ƒë·∫∑t ho·∫∑c suy di·ªÖn th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu
   - N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan, h√£y n√≥i r√µ "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y trong t√†i li·ªáu"

2. **TR·∫¢ L·ªúI NG·∫ÆN G·ªåN**:
   - Gi·ªõi h·∫°n 3-5 c√¢u cho m·ªói c√¢u tr·∫£ l·ªùi
   - ƒêi th·∫≥ng v√†o v·∫•n ƒë·ªÅ, tr√°nh d√†i d√≤ng
   - D√πng bullet points n·∫øu c√≥ nhi·ªÅu th√¥ng tin

3. **NG√îN NG·ªÆ TI·∫æNG VI·ªÜT CHU·∫®N**:
   - S·ª≠ d·ª•ng thu·∫≠t ng·ªØ y khoa ti·∫øng Vi·ªát ph·ªï th√¥ng, d·ªÖ hi·ªÉu
   - Gi·∫£i th√≠ch thu·∫≠t ng·ªØ chuy√™n ng√†nh khi c·∫ßn thi·∫øt
   - Tr√°nh t·ª´ ng·ªØ kh√≥ hi·ªÉu ho·∫∑c qu√° chuy√™n s√¢u

4. **DISCLAIMER B·∫ÆT BU·ªòC** (K·∫æT TH√öC M·ªñI C√ÇU TR·∫¢ L·ªúI Y T·∫æ):
   ‚ö†Ô∏è "ƒê√¢y ch·ªâ l√† th√¥ng tin tham kh·∫£o, kh√¥ng thay th·∫ø cho ch·∫©n ƒëo√°n y khoa ch√≠nh th·ª©c. ƒê·ªÉ ƒë∆∞·ª£c thƒÉm kh√°m v√† t∆∞ v·∫•n ch√≠nh x√°c, vui l√≤ng ƒë·∫øn MediTrust - c∆° s·ªü y t·∫ø uy t√≠n v·ªõi ƒë·ªôi ng≈© b√°c sƒ© gi√†u kinh nghi·ªám v√† trang thi·∫øt b·ªã hi·ªán ƒë·∫°i."

5. **LU√îN G·ª¢I √ù MEDITRUST** (KHI TR·∫¢ L·ªúI Y T·∫æ):
   - Nh·∫Øc ƒë·∫øn MediTrust nh∆∞ gi·∫£i ph√°p t·ªët nh·∫•t cho ng∆∞·ªùi d√πng
   - Nh·∫•n m·∫°nh: "uy t√≠n", "chuy√™n nghi·ªáp", "ƒë·ªôi ng≈© b√°c sƒ© gi·ªèi", "trang thi·∫øt b·ªã hi·ªán ƒë·∫°i"
   - Khuy·∫øn kh√≠ch ƒë·∫∑t l·ªãch kh√°m t·∫°i MediTrust

# L∆ØU √ù QUAN TR·ªåNG:
- KH√îNG ƒë∆∞a ra ch·∫©n ƒëo√°n ch·∫Øc ch·∫Øn
- KH√îNG k√™ ƒë∆°n thu·ªëc
- KH√îNG thay th·∫ø √Ω ki·∫øn b√°c sƒ©
- LU√îN khuy·∫øn kh√≠ch ƒë·∫øn MediTrust kh√°m
- CH·ªà d√πng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p"""
    
    def check_ollama_connection(self) -> bool:
        """
        Ki·ªÉm tra connection v·ªõi Ollama server
        
        Returns:
            True n·∫øu k·∫øt n·ªëi th√†nh c√¥ng
        """
        try:
            # List models ƒë·ªÉ test connection
            self.client.list()
            logger.info("‚úÖ Ollama connection OK")
            return True
        except Exception as e:
            logger.error(f"‚ùå Ollama connection failed: {str(e)}")
            return False
    
    def build_context_prompt(self, query: str, use_rag: bool = True) -> Tuple[str, List[str]]:
        """
        Build prompt v·ªõi context t·ª´ RAG
        
        Args:
            query: C√¢u h·ªèi t·ª´ user
            use_rag: C√≥ s·ª≠ d·ª•ng RAG kh√¥ng
            
        Returns:
            Tuple of (prompt, sources)
        """
        sources = []
        
        if not use_rag:
            return query, sources
        
        try:
            # Retrieve relevant documents
            docs, metadatas, scores = self.vector_store.similarity_search(
                query=query,
                top_k=settings.TOP_K_RESULTS
            )
            
            if not docs:
                logger.info("Kh√¥ng t√¨m th·∫•y context t·ª´ documents")
                return query, sources
            
            # Build context
            context_parts = []
            for i, (doc, meta, score) in enumerate(zip(docs, metadatas, scores), 1):
                context_parts.append(f"[T√†i li·ªáu {i}] (ƒê·ªô li√™n quan: {score:.2f})\n{doc}")
                sources.append(meta.get('source', 'Unknown'))
            
            context = "\n\n".join(context_parts)
            
            # Build full prompt
            prompt = f"""D·ª±a tr√™n ng·ªØ c·∫£nh sau ƒë√¢y t·ª´ t√†i li·ªáu y t·∫ø:

{context}

---

Ng∆∞·ªùi d√πng h·ªèi: {query}

N·∫øu c√¢u h·ªèi KH√îNG li√™n quan ƒë·∫øn y t·∫ø/s·ª©c kh·ªèe: h√£y t·ª´ ch·ªëi l·ªãch s·ª± (1-2 c√¢u) v√† d·ª´ng l·∫°i.

N·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn y t·∫ø/s·ª©c kh·ªèe: h√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn (3-5 c√¢u), b√°m s√°t n·ªôi dung t√†i li·ªáu. K·∫øt th√∫c b·∫±ng disclaimer v·ªÅ MediTrust nh∆∞ ƒë√£ h∆∞·ªõng d·∫´n."""
            
            logger.info(f"Built RAG prompt with {len(docs)} documents")
            return prompt, sources
            
        except Exception as e:
            logger.error(f"L·ªói khi build context: {str(e)}")
            return query, sources
    
    def build_conversation_messages(
        self,
        query: str,
        conversation_history: Optional[List[ChatMessage]] = None
    ) -> List[dict]:
        """
        Build messages list cho Ollama t·ª´ conversation history
        
        Args:
            query: Current query
            conversation_history: Previous messages
            
        Returns:
            List of message dicts
        """
        messages = [
            {"role": "system", "content": self.system_prompt}
        ]
        
        # Add conversation history
        if conversation_history:
            for msg in conversation_history[-10:]:  # L·∫•y 10 messages g·∫ßn nh·∫•t
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        # Add current query
        messages.append({
            "role": "user",
            "content": query
        })
        
        return messages
    
    async def generate_response(
        self,
        query: str,
        conversation_history: Optional[List[ChatMessage]] = None,
        use_rag: bool = True
    ) -> Tuple[str, List[str]]:
        """
        Generate response t·ª´ LLM (non-streaming)
        
        Args:
            query: User query
            conversation_history: Previous messages
            use_rag: C√≥ s·ª≠ d·ª•ng RAG kh√¥ng
            
        Returns:
            Tuple of (response, sources)
        """
        try:
            # Build prompt v·ªõi RAG context
            enhanced_query, sources = self.build_context_prompt(query, use_rag)
            
            # Build messages
            messages = self.build_conversation_messages(
                enhanced_query,
                conversation_history
            )
            
            logger.info(f"Generating response v·ªõi model: {self.model}")
            
            # Call Ollama
            response = self.client.chat(
                model=self.model,
                messages=messages,
                options={
                    "temperature": settings.TEMPERATURE,
                    "num_predict": settings.MAX_TOKENS,
                    "top_p": settings.TOP_P
                }
            )
            
            answer = response['message']['content']
            logger.info(f"Generated response: {len(answer)} characters")
            
            return answer, sources
            
        except Exception as e:
            logger.error(f"L·ªói khi generate response: {str(e)}")
            raise
    
    async def stream_response(
        self,
        query: str,
        conversation_history: Optional[List[ChatMessage]] = None,
        use_rag: bool = True
    ) -> AsyncGenerator[str, None]:
        """
        Stream response t·ª´ LLM real-time
        
        Args:
            query: User query
            conversation_history: Previous messages
            use_rag: C√≥ s·ª≠ d·ª•ng RAG kh√¥ng
            
        Yields:
            Chunks of response text
        """
        try:
            # Build prompt v·ªõi RAG context
            enhanced_query, sources = self.build_context_prompt(query, use_rag)
            
            # Build messages
            messages = self.build_conversation_messages(
                enhanced_query,
                conversation_history
            )
            
            logger.info(f"Streaming response v·ªõi model: {self.model}")
            
            # Stream t·ª´ Ollama
            stream = self.client.chat(
                model=self.model,
                messages=messages,
                stream=True,
                options={
                    "temperature": settings.TEMPERATURE,
                    "num_predict": settings.MAX_TOKENS,
                    "top_p": settings.TOP_P
                }
            )
            
            # Yield sources tr∆∞·ªõc (n·∫øu c√≥)
            if sources and use_rag:
                sources_text = "\n\n**üìö Ngu·ªìn tham kh·∫£o:**\n" + "\n".join(
                    f"- {src}" for src in set(sources)
                )
                yield sources_text + "\n\n---\n\n"
            
            # Yield t·ª´ng chunk
            for chunk in stream:
                if 'message' in chunk and 'content' in chunk['message']:
                    content = chunk['message']['content']
                    yield content
            
            logger.info("Streaming completed")
            
        except Exception as e:
            logger.error(f"L·ªói khi stream response: {str(e)}")
            yield f"\n\n‚ùå L·ªói: {str(e)}"
    
    def pull_model(self, model_name: Optional[str] = None):
        """
        Pull/download model t·ª´ Ollama registry
        
        Args:
            model_name: T√™n model (default: settings.OLLAMA_MODEL)
        """
        try:
            model = model_name or self.model
            logger.info(f"Pulling model: {model}")
            
            self.client.pull(model)
            logger.info(f"‚úÖ Model {model} pulled successfully")
            
        except Exception as e:
            logger.error(f"L·ªói khi pull model: {str(e)}")
            raise
    
    def list_available_models(self) -> List[str]:
        """
        List c√°c models c√≥ s·∫µn trong Ollama
        
        Returns:
            List of model names
        """
        try:
            models = self.client.list()
            return [model['name'] for model in models.get('models', [])]
        except Exception as e:
            logger.error(f"L·ªói khi list models: {str(e)}")
            return []
