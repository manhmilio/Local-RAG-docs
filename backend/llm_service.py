"""
LLM Service Module - T√≠ch h·ª£p Ollama v·ªõi LangChain v√† RAG pipeline
"""
import ollama
from typing import List, Optional, Tuple, AsyncGenerator
import logging
from config import settings
from models import ChatMessage
from vector_store import VectorStore

logger = logging.getLogger(__name__)


class LLMService:
    """
    Service x·ª≠ l√Ω LLM requests v·ªõi Ollama v√† RAG
    """
    
    def __init__(self, vector_store: VectorStore):
        """
        Args:
            vector_store: Instance c·ªßa VectorStore ƒë·ªÉ retrieve context
        """
        self.vector_store = vector_store
        self.model = settings.OLLAMA_MODEL
        self.client = ollama.Client(host=settings.OLLAMA_BASE_URL)
        
        # System prompt cho medical chatbot
        self.system_prompt = """B·∫°n l√† m·ªôt tr·ª£ l√Ω y t·∫ø AI th√¥ng minh v√† chuy√™n nghi·ªáp. 
Nhi·ªám v·ª• c·ªßa b·∫°n l√†:
1. Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ tri·ªáu ch·ª©ng v√† b·ªánh t·∫≠t d·ª±a tr√™n th√¥ng tin y t·∫ø ƒë∆∞·ª£c cung c·∫•p
2. Gi·∫£i th√≠ch r√µ r√†ng, d·ªÖ hi·ªÉu b·∫±ng ti·∫øng Vi·ªát
3. Lu√¥n khuy√™n ng∆∞·ªùi d√πng n√™n g·∫∑p b√°c sƒ© ƒë·ªÉ ch·∫©n ƒëo√°n ch√≠nh x√°c
4. KH√îNG t·ª± √Ω ch·∫©n ƒëo√°n ho·∫∑c k√™ ƒë∆°n thu·ªëc
5. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y th·ª´a nh·∫≠n v√† ƒë·ªÅ ngh·ªã t√¨m ki·∫øm √Ω ki·∫øn chuy√™n gia

H√£y tr·∫£ l·ªùi m·ªôt c√°ch th√¢n thi·ªán, c√≥ c·∫•u tr√∫c, v√† chuy√™n nghi·ªáp."""
    
    def check_ollama_connection(self) -> bool:
        """
        Ki·ªÉm tra connection v·ªõi Ollama server
        
        Returns:
            True n·∫øu k·∫øt n·ªëi th√†nh c√¥ng
        """
        try:
            # List models ƒë·ªÉ test connection
            self.client.list()
            logger.info("‚úÖ Ollama connection OK")
            return True
        except Exception as e:
            logger.error(f"‚ùå Ollama connection failed: {str(e)}")
            return False
    
    def build_context_prompt(self, query: str, use_rag: bool = True) -> Tuple[str, List[str]]:
        """
        Build prompt v·ªõi context t·ª´ RAG
        
        Args:
            query: C√¢u h·ªèi t·ª´ user
            use_rag: C√≥ s·ª≠ d·ª•ng RAG kh√¥ng
            
        Returns:
            Tuple of (prompt, sources)
        """
        sources = []
        
        if not use_rag:
            return query, sources
        
        try:
            # Retrieve relevant documents
            docs, metadatas, scores = self.vector_store.similarity_search(
                query=query,
                top_k=settings.TOP_K_RESULTS
            )
            
            if not docs:
                logger.info("Kh√¥ng t√¨m th·∫•y context t·ª´ documents")
                return query, sources
            
            # Build context
            context_parts = []
            for i, (doc, meta, score) in enumerate(zip(docs, metadatas, scores), 1):
                context_parts.append(f"[T√†i li·ªáu {i}] (ƒê·ªô li√™n quan: {score:.2f})\n{doc}")
                sources.append(meta.get('source', 'Unknown'))
            
            context = "\n\n".join(context_parts)
            
            # Build full prompt
            prompt = f"""D·ª±a tr√™n c√°c th√¥ng tin y t·∫ø sau:

{context}

---

C√¢u h·ªèi: {query}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p ·ªü tr√™n. N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥."""
            
            logger.info(f"Built RAG prompt with {len(docs)} documents")
            return prompt, sources
            
        except Exception as e:
            logger.error(f"L·ªói khi build context: {str(e)}")
            return query, sources
    
    def build_conversation_messages(
        self,
        query: str,
        conversation_history: Optional[List[ChatMessage]] = None
    ) -> List[dict]:
        """
        Build messages list cho Ollama t·ª´ conversation history
        
        Args:
            query: Current query
            conversation_history: Previous messages
            
        Returns:
            List of message dicts
        """
        messages = [
            {"role": "system", "content": self.system_prompt}
        ]
        
        # Add conversation history
        if conversation_history:
            for msg in conversation_history[-10:]:  # L·∫•y 10 messages g·∫ßn nh·∫•t
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        # Add current query
        messages.append({
            "role": "user",
            "content": query
        })
        
        return messages
    
    async def generate_response(
        self,
        query: str,
        conversation_history: Optional[List[ChatMessage]] = None,
        use_rag: bool = True
    ) -> Tuple[str, List[str]]:
        """
        Generate response t·ª´ LLM (non-streaming)
        
        Args:
            query: User query
            conversation_history: Previous messages
            use_rag: C√≥ s·ª≠ d·ª•ng RAG kh√¥ng
            
        Returns:
            Tuple of (response, sources)
        """
        try:
            # Build prompt v·ªõi RAG context
            enhanced_query, sources = self.build_context_prompt(query, use_rag)
            
            # Build messages
            messages = self.build_conversation_messages(
                enhanced_query,
                conversation_history
            )
            
            logger.info(f"Generating response v·ªõi model: {self.model}")
            
            # Call Ollama
            response = self.client.chat(
                model=self.model,
                messages=messages,
                options={
                    "temperature": settings.TEMPERATURE,
                    "num_predict": settings.MAX_TOKENS,
                    "top_p": settings.TOP_P
                }
            )
            
            answer = response['message']['content']
            logger.info(f"Generated response: {len(answer)} characters")
            
            return answer, sources
            
        except Exception as e:
            logger.error(f"L·ªói khi generate response: {str(e)}")
            raise
    
    async def stream_response(
        self,
        query: str,
        conversation_history: Optional[List[ChatMessage]] = None,
        use_rag: bool = True
    ) -> AsyncGenerator[str, None]:
        """
        Stream response t·ª´ LLM real-time
        
        Args:
            query: User query
            conversation_history: Previous messages
            use_rag: C√≥ s·ª≠ d·ª•ng RAG kh√¥ng
            
        Yields:
            Chunks of response text
        """
        try:
            # Build prompt v·ªõi RAG context
            enhanced_query, sources = self.build_context_prompt(query, use_rag)
            
            # Build messages
            messages = self.build_conversation_messages(
                enhanced_query,
                conversation_history
            )
            
            logger.info(f"Streaming response v·ªõi model: {self.model}")
            
            # Stream t·ª´ Ollama
            stream = self.client.chat(
                model=self.model,
                messages=messages,
                stream=True,
                options={
                    "temperature": settings.TEMPERATURE,
                    "num_predict": settings.MAX_TOKENS,
                    "top_p": settings.TOP_P
                }
            )
            
            # Yield sources tr∆∞·ªõc (n·∫øu c√≥)
            if sources and use_rag:
                sources_text = "\n\n**üìö Ngu·ªìn tham kh·∫£o:**\n" + "\n".join(
                    f"- {src}" for src in set(sources)
                )
                yield sources_text + "\n\n---\n\n"
            
            # Yield t·ª´ng chunk
            for chunk in stream:
                if 'message' in chunk and 'content' in chunk['message']:
                    content = chunk['message']['content']
                    yield content
            
            logger.info("Streaming completed")
            
        except Exception as e:
            logger.error(f"L·ªói khi stream response: {str(e)}")
            yield f"\n\n‚ùå L·ªói: {str(e)}"
    
    def pull_model(self, model_name: Optional[str] = None):
        """
        Pull/download model t·ª´ Ollama registry
        
        Args:
            model_name: T√™n model (default: settings.OLLAMA_MODEL)
        """
        try:
            model = model_name or self.model
            logger.info(f"Pulling model: {model}")
            
            self.client.pull(model)
            logger.info(f"‚úÖ Model {model} pulled successfully")
            
        except Exception as e:
            logger.error(f"L·ªói khi pull model: {str(e)}")
            raise
    
    def list_available_models(self) -> List[str]:
        """
        List c√°c models c√≥ s·∫µn trong Ollama
        
        Returns:
            List of model names
        """
        try:
            models = self.client.list()
            return [model['name'] for model in models.get('models', [])]
        except Exception as e:
            logger.error(f"L·ªói khi list models: {str(e)}")
            return []
