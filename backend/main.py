"""
Main FastAPI Application - Entry point c·ªßa backend
"""
from fastapi import FastAPI, UploadFile, File, HTTPException, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from contextlib import asynccontextmanager
import logging
from typing import AsyncGenerator, Optional
import json

from config import settings
from models import (
    ChatRequest, 
    ChatResponse, 
    HealthResponse,
    DocumentUploadResponse,
    EmbeddingStats
)
from vector_store import VectorStore
from llm_service import LLMService
from pdf_processor import PDFProcessor
from auth import verify_api_key, optional_verify_api_key
from rate_limiter import check_rate_limit, rate_limiter

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Global instances
vector_store: VectorStore = None
llm_service: LLMService = None
pdf_processor: PDFProcessor = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifecycle management - kh·ªüi t·∫°o v√† cleanup resources"""
    global vector_store, llm_service, pdf_processor
    
    logger.info("üöÄ Kh·ªüi ƒë·ªông ·ª©ng d·ª•ng Medical Chatbot...")
    
    try:
        # Kh·ªüi t·∫°o Vector Store
        logger.info("ƒêang kh·ªüi t·∫°o ChromaDB Vector Store...")
        vector_store = VectorStore()
        
        # Kh·ªüi t·∫°o LLM Service
        logger.info(f"ƒêang k·∫øt n·ªëi Ollama v·ªõi model: {settings.OLLAMA_MODEL}...")
        llm_service = LLMService(vector_store)
        
        # Kh·ªüi t·∫°o PDF Processor
        logger.info("ƒêang kh·ªüi t·∫°o PDF Processor...")
        pdf_processor = PDFProcessor(vector_store)
        
        logger.info("‚úÖ Kh·ªüi ƒë·ªông th√†nh c√¥ng!")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi kh·ªüi ƒë·ªông: {str(e)}")
        raise
    
    yield
    
    # Cleanup
    logger.info("üõë ƒêang d·ª´ng ·ª©ng d·ª•ng...")


# Kh·ªüi t·∫°o FastAPI app
app = FastAPI(
    title="Medical Chatbot API",
    description="API cho h·ªá th·ªëng chatbot ch·∫©n ƒëo√°n b·ªánh v·ªõi RAG. H·ªó tr·ª£ k·∫øt n·ªëi t·ª´ b·∫•t k·ª≥ d·ª± √°n n√†o th√¥ng qua REST API.",
    version="1.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins_list,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["X-RateLimit-Limit", "X-RateLimit-Remaining", "X-RateLimit-Reset"]
)


@app.get("/", tags=["Root"])
async def root():
    """Root endpoint"""
    return {
        "message": "Medical Chatbot API",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health",
        "endpoints": {
            "chat": "/chat",
            "chat_stream": "/chat/stream",
            "upload": "/documents/upload",
            "stats": "/documents/stats"
        },
        "authentication": settings.ENABLE_API_KEY_AUTH,
        "rate_limiting": settings.ENABLE_RATE_LIMITING
    }


@app.get("/api/info", tags=["Root"])
async def api_info():
    """
    L·∫•y th√¥ng tin chi ti·∫øt v·ªÅ API v√† c·∫•u h√¨nh h·ªá th·ªëng
    """
    return {
        "api_version": "1.0.0",
        "model": settings.OLLAMA_MODEL,
        "embedding_model": settings.EMBEDDING_MODEL,
        "features": {
            "rag_enabled": True,
            "streaming_support": True,
            "document_upload": True,
            "multilingual": True
        },
        "limits": {
            "max_tokens": settings.MAX_TOKENS,
            "rate_limit_per_minute": settings.RATE_LIMIT_PER_MINUTE if settings.ENABLE_RATE_LIMITING else None,
            "rate_limit_per_hour": settings.RATE_LIMIT_PER_HOUR if settings.ENABLE_RATE_LIMITING else None
        },
        "authentication": {
            "required": settings.ENABLE_API_KEY_AUTH,
            "method": "API Key (Header: X-API-Key)" if settings.ENABLE_API_KEY_AUTH else "None"
        },
        "cors": {
            "allow_all_origins": settings.ALLOW_ALL_ORIGINS,
            "allowed_origins": settings.cors_origins_list if not settings.ALLOW_ALL_ORIGINS else ["*"]
        }
    }


@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """
    Health check endpoint - ki·ªÉm tra tr·∫°ng th√°i h·ªá th·ªëng
    """
    try:
        ollama_status = llm_service.check_ollama_connection()
        chroma_status = vector_store is not None
        
        return HealthResponse(
            status="healthy" if (ollama_status and chroma_status) else "degraded",
            ollama_connected=ollama_status,
            chroma_initialized=chroma_status
        )
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        raise HTTPException(status_code=503, detail=str(e))


@app.post("/chat", response_model=ChatResponse, tags=["Chat"])
async def chat(
    request: ChatRequest,
    req: Request,
    api_key: str = Depends(optional_verify_api_key)
):
    """
    Chat endpoint - x·ª≠ l√Ω c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng
    
    Args:
        request: ChatRequest ch·ª©a message v√† l·ªãch s·ª≠ h·ªôi tho·∫°i
        
    Returns:
        ChatResponse v·ªõi c√¢u tr·∫£ l·ªùi v√† sources
        
    Headers:
        X-API-Key: API key for authentication (if enabled)
    """
    # Check rate limit
    if settings.ENABLE_RATE_LIMITING:
        await check_rate_limit(req)
    
    try:
        logger.info(f"Nh·∫≠n c√¢u h·ªèi: {request.message[:100]}...")
        
        # G·ªçi LLM service ƒë·ªÉ x·ª≠ l√Ω
        response, sources = await llm_service.generate_response(
            query=request.message,
            conversation_history=request.conversation_history,
            use_rag=request.use_rag
        )
        
        return ChatResponse(
            response=response,
            sources=sources if request.use_rag else []
        )
        
    except Exception as e:
        logger.error(f"L·ªói khi x·ª≠ l√Ω chat: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω: {str(e)}")


@app.post("/chat/stream", tags=["Chat"])
async def chat_stream(
    request: ChatRequest,
    req: Request,
    api_key: str = Depends(optional_verify_api_key)
):
    """
    Streaming chat endpoint - tr·∫£ v·ªÅ response theo real-time
    
    Returns:
        StreamingResponse v·ªõi Server-Sent Events (SSE)
        
    Headers:
        X-API-Key: API key for authentication (if enabled)
    """
    # Check rate limit
    if settings.ENABLE_RATE_LIMITING:
        await check_rate_limit(req)
    
    async def generate_stream() -> AsyncGenerator[str, None]:
        try:
            async for chunk in llm_service.stream_response(
                query=request.message,
                conversation_history=request.conversation_history,
                use_rag=request.use_rag
            ):
                # Format as SSE
                yield f"data: {json.dumps({'chunk': chunk})}\n\n"
                
        except Exception as e:
            logger.error(f"L·ªói streaming: {str(e)}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )


@app.post("/documents/upload", response_model=DocumentUploadResponse, tags=["Documents"])
async def upload_document(
    file: UploadFile = File(...),
    api_key: str = Depends(verify_api_key)
):
    """
    Upload v√† x·ª≠ l√Ω t√†i li·ªáu PDF y t·∫ø
    
    Args:
        file: PDF file upload
        
    Returns:
        DocumentUploadResponse v·ªõi th√¥ng tin x·ª≠ l√Ω
        
    Headers:
        X-API-Key: API key for authentication (required if auth is enabled)
    """
    try:
        # Ki·ªÉm tra file type
        if not file.filename.endswith('.pdf'):
            raise HTTPException(
                status_code=400,
                detail="Ch·ªâ ch·∫•p nh·∫≠n file PDF"
            )
        
        logger.info(f"ƒêang x·ª≠ l√Ω file: {file.filename}")
        
        # ƒê·ªçc file content
        content = await file.read()
        
        # Process PDF
        chunks_created = await pdf_processor.process_pdf(
            content=content,
            filename=file.filename
        )
        
        return DocumentUploadResponse(
            filename=file.filename,
            chunks_created=chunks_created,
            status="success",
            message=f"ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng {chunks_created} chunks t·ª´ {file.filename}"
        )
        
    except Exception as e:
        logger.error(f"L·ªói khi upload document: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/documents/stats", response_model=EmbeddingStats, tags=["Documents"])
async def get_document_stats():
    """
    L·∫•y th·ªëng k√™ v·ªÅ t√†i li·ªáu trong vector store
    """
    try:
        stats = vector_store.get_stats()
        return EmbeddingStats(**stats)
    except Exception as e:
        logger.error(f"L·ªói khi l·∫•y stats: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/documents/reindex", tags=["Documents"])
async def reindex_documents(api_key: str = Depends(verify_api_key)):
    """
    Reindex t·∫•t c·∫£ PDF files trong th∆∞ m·ª•c data
    
    Headers:
        X-API-Key: API key for authentication (required if auth is enabled)
    """
    try:
        logger.info("B·∫Øt ƒë·∫ßu reindex documents...")
        result = await pdf_processor.reindex_all_pdfs()
        return JSONResponse(content=result)
    except Exception as e:
        logger.error(f"L·ªói khi reindex: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.RELOAD
    )
