#!/usr/bin/env python3
"""
Script Ä‘á»ƒ test cÃ¡c components cá»§a há»‡ thá»‘ng
"""
import sys
import asyncio
from pathlib import Path

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent))

from config import settings
from vector_store import VectorStore
from llm_service import LLMService
from pdf_processor import PDFProcessor


async def test_vector_store():
    """Test ChromaDB vÃ  embeddings"""
    print("\nğŸ§ª Testing VectorStore...")
    
    try:
        vs = VectorStore()
        
        # Test embedding
        texts = ["Bá»‡nh cáº£m cÃºm", "Triá»‡u chá»©ng sá»‘t"]
        embeddings = vs.embed_texts(texts)
        print(f"âœ… Embeddings: {len(embeddings)} vectors")
        
        # Test add documents
        count = vs.add_documents(
            texts=texts,
            metadatas=[{"source": "test1"}, {"source": "test2"}]
        )
        print(f"âœ… Added {count} documents")
        
        # Test search
        docs, metas, scores = vs.similarity_search("triá»‡u chá»©ng", top_k=2)
        print(f"âœ… Search results: {len(docs)} documents")
        
        return True
    except Exception as e:
        print(f"âŒ VectorStore test failed: {str(e)}")
        return False


async def test_llm_service():
    """Test Ollama connection"""
    print("\nğŸ§ª Testing LLM Service...")
    
    try:
        vs = VectorStore()
        llm = LLMService(vs)
        
        # Test connection
        is_connected = llm.check_ollama_connection()
        print(f"{'âœ…' if is_connected else 'âŒ'} Ollama connection: {is_connected}")
        
        # Test list models
        models = llm.list_available_models()
        print(f"âœ… Available models: {models}")
        
        return is_connected
    except Exception as e:
        print(f"âŒ LLM Service test failed: {str(e)}")
        return False


async def test_pdf_processor():
    """Test PDF processing"""
    print("\nğŸ§ª Testing PDF Processor...")
    
    try:
        vs = VectorStore()
        processor = PDFProcessor(vs)
        
        # Test text cleaning
        text = "  Multiple   spaces   here  "
        cleaned = processor.clean_text(text)
        print(f"âœ… Text cleaning works: '{cleaned}'")
        
        # Test chunking
        long_text = "Lorem ipsum. " * 100
        chunks = processor.chunk_text(long_text, {"source": "test"})
        print(f"âœ… Created {len(chunks)} chunks")
        
        return True
    except Exception as e:
        print(f"âŒ PDF Processor test failed: {str(e)}")
        return False


async def test_end_to_end():
    """Test full RAG pipeline"""
    print("\nğŸ§ª Testing End-to-End RAG Pipeline...")
    
    try:
        # Setup
        vs = VectorStore()
        llm = LLMService(vs)
        
        # Add test documents
        docs = [
            "Bá»‡nh cáº£m cÃºm lÃ  bá»‡nh nhiá»…m trÃ¹ng Ä‘Æ°á»ng hÃ´ háº¥p do virus cÃºm gÃ¢y ra. CÃ¡c triá»‡u chá»©ng bao gá»“m sá»‘t, ho, Ä‘au há»ng.",
            "Triá»‡u chá»©ng thÆ°á»ng gáº·p cá»§a cáº£m cÃºm: sá»‘t cao, Ä‘au Ä‘áº§u, má»‡t má»i, ho khÃ´, Ä‘au cÆ¡.",
            "Äá»ƒ phÃ²ng ngá»«a cáº£m cÃºm, nÃªn tiÃªm vaccine hÃ ng nÄƒm vÃ  rá»­a tay thÆ°á»ng xuyÃªn."
        ]
        
        vs.add_documents(
            texts=docs,
            metadatas=[{"source": f"test_doc_{i}"} for i in range(len(docs))]
        )
        print("âœ… Added test documents")
        
        # Test query
        response, sources = await llm.generate_response(
            query="Triá»‡u chá»©ng cá»§a cáº£m cÃºm lÃ  gÃ¬?",
            use_rag=True
        )
        
        print(f"âœ… Generated response: {len(response)} characters")
        print(f"âœ… Sources: {sources}")
        print(f"\nğŸ“ Response preview:\n{response[:200]}...")
        
        return True
    except Exception as e:
        print(f"âŒ End-to-end test failed: {str(e)}")
        return False


async def main():
    """Run all tests"""
    print("=" * 60)
    print("ğŸš€ Medical Chatbot - System Tests")
    print("=" * 60)
    
    print(f"\nâš™ï¸  Configuration:")
    print(f"   - Ollama Model: {settings.OLLAMA_MODEL}")
    print(f"   - Ollama URL: {settings.OLLAMA_BASE_URL}")
    print(f"   - Embedding Model: {settings.EMBEDDING_MODEL}")
    print(f"   - ChromaDB: {settings.CHROMA_PERSIST_DIRECTORY}")
    
    results = {}
    
    # Run tests
    results["VectorStore"] = await test_vector_store()
    results["LLM Service"] = await test_llm_service()
    results["PDF Processor"] = await test_pdf_processor()
    
    if all([results["VectorStore"], results["LLM Service"]]):
        results["End-to-End"] = await test_end_to_end()
    else:
        print("\nâš ï¸  Skipping end-to-end test due to previous failures")
        results["End-to-End"] = False
    
    # Summary
    print("\n" + "=" * 60)
    print("ğŸ“Š Test Results Summary:")
    print("=" * 60)
    
    for test_name, result in results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"   {test_name}: {status}")
    
    all_passed = all(results.values())
    print("\n" + "=" * 60)
    if all_passed:
        print("ğŸ‰ All tests passed!")
    else:
        print("âš ï¸  Some tests failed. Check output above.")
    print("=" * 60)
    
    return 0 if all_passed else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
